{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef66a4d-5b0a-4a3c-87da-acd9b4893320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14dbd38-3b5c-4334-ab68-b98178e47744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/bakhi/.cache/huggingface/datasets/bakhitovd___json/bakhitovd--data_science_arxiv-d562cf23e63fbcaf/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    }
   ],
   "source": [
    "data_test = load_dataset('bakhitovd/data_science_arxiv', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9231880e-1ae1-48dd-9f49-7669fbabadbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(batch):\n",
    "    inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=10240, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    # put global attention on <s> token\n",
    "    global_attention_mask[:, 0] = 1\n",
    "    \n",
    "    predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask, max_length=512)\n",
    "    batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d6963-164c-41e9-abf1-dc150e6c9b11",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a26cc739-9939-4406-83e1-cbb0b7dfb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"checkpoint-22500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32843c96-9226-4e40-8b4c-8cb58f9b209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"checkpoint-22500\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c003f7d-5fd0-47d7-be69-c72942ef718d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.43337361986424644\n",
      "rouge2:  0.1629855271710752\n",
      "rougeL:  0.2459445128507289\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91ed8a-f47e-4a53-831d-bbb202f2ea72",
   "metadata": {},
   "source": [
    "# allenai/led-large-16384-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38bfd7c-6c41-44b7-a718-64916ae54b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccc69bda-184b-4d61-828c-a17b6b296f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a8f568-758d-4d49-bcca-27b9b929334b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380a2409c1ec4fa19817e580b1cfa612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 8.00 GiB total capacity; 5.27 GiB already allocated; 0 bytes free; 6.86 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d6b6f920900b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerate_answer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrouge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rouge\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mrouge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"predicted_abstract\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"rouge1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rouge2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rougeL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rouge1: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rouge1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmeasure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rouge2: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rouge2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmeasure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Dataset\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"self\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m         }\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;31m# re-apply format to the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3002\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"Map\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3003\u001b[0m                 ) as pbar:\n\u001b[1;32m-> 3004\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3005\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3006\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3378\u001b[0m                         )  # Something simpler?\n\u001b[0;32m   3379\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3380\u001b[1;33m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[0;32m   3381\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3382\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3259\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3260\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3261\u001b[1;33m             \u001b[0mprocessed_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3262\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3263\u001b[0m                 processed_inputs = {\n",
      "\u001b[1;32m<ipython-input-4-89568e12f00e>\u001b[0m in \u001b[0;36mgenerate_answer\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mglobal_attention_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpredicted_abstract_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"predicted_abstract\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_abstract_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1365\u001b[0m             \u001b[1;31m# if model is encoder decoder encoder_outputs are created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[1;31m# and added to `model_kwargs`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[0;32m   1368\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"return_dict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"encoder_outputs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1895\u001b[0m                     )\n\u001b[0;32m   1896\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1897\u001b[1;33m                     layer_outputs = encoder_layer(\n\u001b[0m\u001b[0;32m   1898\u001b[0m                         \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[0;32m    972\u001b[0m         \"\"\"\n\u001b[0;32m    973\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 974\u001b[1;33m         attn_outputs = self.self_attn(\n\u001b[0m\u001b[0;32m    975\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[1;34m\"\"\"Input shape: Batch x Time x Channel\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m         self_outputs = self.longformer_self_attn(\n\u001b[0m\u001b[0;32m    784\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\led\\modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;31m# softmax sometimes inserts NaN if all positions are masked, replace them with 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[0mattn_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_index_masked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[0mattn_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 8.00 GiB total capacity; 5.27 GiB already allocated; 0 bytes free; 6.86 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=2)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad927f-6253-4016-a808-ebfdba3b6215",
   "metadata": {},
   "source": [
    "led-large-16384-arxiv | 0.436322167 | 0.168023514 | 0.245217045"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c8be49-817e-4a9c-86aa-43095a67fda1",
   "metadata": {},
   "source": [
    "# allenai/led-large-16384-arxiv_512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46892d3-0afd-43a9-9829-53a24a8e487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb9e3b22-6dec-473c-842f-925c25a3400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0969d33e-ede2-4c9d-8f39-8a9716eeaea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-d6b6f920900b>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.43504442225757034\n",
      "rouge2:  0.17124051696063441\n",
      "rougeL:  0.24651911996386372\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=2)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565dc1d9-2b1b-422c-b56d-8cf6e40b1259",
   "metadata": {},
   "source": [
    "# NielsV/led-arxiv-10240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc0d428-e974-4ad9-983d-c62eb578d227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b1a310bcd44910b897a12fb4d19df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bakhi\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bakhi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2a9b170608416dafeee1c90f003cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b999d58bfa48a89140a95ab94ad266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3e39049aef4da9a16b8ced1f0bf5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"NielsV/led-arxiv-10240\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ef439a-d228-42d3-95b7-a6b0433b8473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21009df4d5bf40768674caa68e32f6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da296332fdd4d1e944f782c4a54c951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"NielsV/led-arxiv-10240\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982c368e-0655-4866-a57a-ec6647ceb9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-d6b6f920900b>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4328558701386347\n",
      "rouge2:  0.16079618784920696\n",
      "rougeL:  0.2386949665758108\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=2)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1010a48-1948-4598-9c62-754ba2ec6003",
   "metadata": {},
   "source": [
    "# ArtifactAI/led_base_16384_arxiv_summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a8a718a-40bf-494f-9c58-cf44f7b67e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91925cd66ccb4b1bb5e41a710d8634eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bakhi\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bakhi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ca22ce14434edf8e9f2e4722f9a152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10834f44a05d4e24b555d8233156cc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6bed03c875407297f90337c2f0c78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"ArtifactAI/led_base_16384_arxiv_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6acb90-6122-4426-b6b8-d0657f7589d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8312ea172b45469499a448e0749243ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ae0091ac814d5e963efe8a87508545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"ArtifactAI/led_base_16384_arxiv_summarization\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c3c4a3-8409-4aa3-a145-e1eb9385dafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-d6b6f920900b>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4267678394989245\n",
      "rouge2:  0.15455973781429677\n",
      "rougeL:  0.23100264873566928\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=2)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731860b9-584d-440e-8495-7007c2d3d09c",
   "metadata": {},
   "source": [
    "# ccdv/lsg-bart-base-16384-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18a4bc2-611b-45e3-89c6-3b5925df016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d42457e-ff36-4fd9-a667-582f93a7019a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc0aca17bc8412daef17fcad4a1f962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-16384-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "490c375d-449f-400a-a2d4-ccaaf151d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523ef544597d4967a891993bea8c1704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ccdv/lsg-bart-base-16384-arxiv\", trust_remote_code=True).to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1450ec52-ae7a-4d96-93b9-ca966f88ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_ccdv(batch):\n",
    "    inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=16384, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
    "    predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask,  max_length=512)\n",
    "    batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bb29827-75ce-4b5d-8b45-fef36b1f8ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at C:\\Users\\bakhi\\.cache\\huggingface\\datasets\\bakhitovd___json\\bakhitovd--data_science_arxiv-d562cf23e63fbcaf\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-da94c11d2a4c8ac9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.42656256464416686\n",
      "rouge2:  0.17277893778163966\n",
      "rougeL:  0.2435920897670595\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer_ccdv, batched=True, batch_size=2)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df2851-c7e5-439c-9956-074c9b9f157f",
   "metadata": {},
   "source": [
    "# ccdv/lsg-bart-base-4096-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a6e55b-7244-4fa5-9c5a-76f1583682de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9765554a-13e7-4728-9fb9-cde54a2ff4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8589ba2c92e4468a9a9f06b75085dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/403 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90aeefc7ddcd407e9919cdb9653c22a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ca33cbed354e86b2840d1a11ac8271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161e00d7c53b43bdac8ace04301db9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72da14deaa8a43a586373a42f71d5b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-4096-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3823200-1003-462a-9552-c71e7953174e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811d5d0dc6224dec864de3c7883bcb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f3976054fe41368b221faaa1fd706e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df8cf7a124147558257fb5db5603254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/578M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ccdv/lsg-bart-base-4096-arxiv\", trust_remote_code=True).to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21ba0020-9289-4c6c-8248-5ca77ce0228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_ccdv(batch):\n",
    "    inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=4096, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
    "    predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask,  max_length=512)\n",
    "    batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cd3e3e7-3bd2-4d3f-91c8-4b98b31345b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.41217009452333453\n",
      "rouge2:  0.16086754601156766\n",
      "rougeL:  0.23435867448479752\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer_ccdv, batched=True, batch_size=8)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02179a-2920-4e9e-ac7e-6a80671c0dcb",
   "metadata": {},
   "source": [
    "# google/pegasus-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f286b496-c32d-4a63-9ac3-847962cbe16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c121ba7-49ff-4ef1-83b3-d6a3a4f90b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-arxiv')\n",
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-arxiv').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca269f3c-4799-4e90-ba80-425b066e9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_pegas(batch):\n",
    "    inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    predicted_abstract_ids = model.generate(input_ids, max_length=512)\n",
    "    batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1758b704-c34a-4724-96c4-8941c864055c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f401afcc29a3>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.39796053226809097\n",
      "rouge2:  0.14236819313001944\n",
      "rougeL:  0.22174633165223157\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer_pegas, batched=True, batch_size=2)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457469d7-fd11-4660-9234-52dce52f01a0",
   "metadata": {},
   "source": [
    "# google/bigbird-pegasus-large-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56676cb-f265-4cee-b882-da0bbecb05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80556f54-b879-4574-9012-5b8946e9caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv', block_size=16, num_random_blocks=2).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a411da-204d-4008-8e4e-6bb99da8e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_bigbird(batch):\n",
    "    inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=4096, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    predicted_abstract_ids = model.generate(input_ids, max_length=512)\n",
    "    batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d282feea-5cee-4d3c-a13e-a3295e606871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bakhi\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "<ipython-input-6-a8cc2223d7e0>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.30521526451025494\n",
      "rouge2:  0.09321141661319818\n",
      "rougeL:  0.19663637267315034\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer_bigbird, batched=True, batch_size=1)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d44a4e-b04b-4840-a651-606e8ca78d0c",
   "metadata": {},
   "source": [
    "# google/bigbird-pegasus-large-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de33415-bbb9-4b5f-b5ae-294d592ab45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22280bd6-efcd-4913-8455-11a0fad9516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv', block_size=32, num_random_blocks=4).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f7e0da-2979-4032-bdee-b45865c40186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_bigbird(batch):\n",
    "    inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=4096, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    predicted_abstract_ids = model.generate(input_ids, max_length=512)\n",
    "    batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c1a83f8-f9c0-47a2-bc44-7ae005a6d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.402266896664768\n",
      "rouge2:  0.1477705948116557\n",
      "rougeL:  0.23141766776089584\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer_bigbird, batched=True, batch_size=1)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bfedcc-639b-4e1b-92a1-79430eaabcfd",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_3.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459131fb-5d70-4980-a0d1-95195d5ef13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"checkpoint-3+1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0cc1ab-1ad0-42f0-88c1-6228bdb4a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"checkpoint-3+1000\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f83f7c0b-422d-433f-8e42-d05bd3eb8c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-434de6d3bf1b>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4347604554981861\n",
      "rouge2:  0.16313799982262556\n",
      "rougeL:  0.2455494779638553\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e24b20-5d32-4746-a30d-bd62371dce83",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4230a8cd-c8a9-4daf-b22d-01e279d019dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"checkpoint-3+2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be2df845-bf92-4acf-bb3e-3485a4a29d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"checkpoint-3+2000\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "696b9578-3943-4062-afdd-f6593919b5a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4339370198531878\n",
      "rouge2:  0.16169393454842412\n",
      "rougeL:  0.24416880991799178\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a111647-c2af-452d-84cf-fa1ec63de4b7",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_3.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cec9afd-c22c-4cc2-99b9-4a742b3e7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"checkpoint-3+3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a24b3f-e397-47a5-a6fc-bce9af7cccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"checkpoint-3+3000\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec395e60-f597-4506-84eb-6e3c5823e8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.43616285781798514\n",
      "rouge2:  0.16268108779101098\n",
      "rougeL:  0.24411254082207115\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fb930-f990-4de5-98d0-9c16cdee5bdc",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4be2b8c-c46d-4d2e-b9e9-f6bd72f2bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"LED_7k_epoch_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "180dc03a-95b2-42bc-af0a-b29b74556990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"LED_7k_epoch_4\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58be717c-815c-4b77-825b-7f01b74252f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4358844669094102\n",
      "rouge2:  0.16334316623445835\n",
      "rougeL:  0.24502806847584777\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5660a-f7c7-4fcc-82ba-c3862613c493",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb878679-a7d9-494b-a65e-777d19735566",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"LED_7k_epoch_4.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265dc839-349e-4dbc-9bfd-2600ac7e6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"LED_7k_epoch_4.5\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bc843be-5cd3-428f-a39b-ebab07493f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4363935623673012\n",
      "rouge2:  0.1632467180044928\n",
      "rougeL:  0.24383670081099068\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc9f10-d2e3-412d-bdfd-df1013adf740",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a2f7c93-5877-4a27-8e55-a2bd438f2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"LED_7k_epoch_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfa7c867-4061-47eb-92ba-6c6caa1daeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"LED_7k_epoch_5\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab6c681-0b9d-466c-bbbe-940086fc35bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-434de6d3bf1b>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4420843777399375\n",
      "rouge2:  0.16586350110140066\n",
      "rougeL:  0.24705680106987288\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23c019-86b6-45ce-b381-3083a83306eb",
   "metadata": {},
   "source": [
    "# LED_16k_epoch_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "450b541d-ae4a-44fa-a0de-9183adecc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"LED_7k_epoch_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bec198c-9b36-4798-af41-ecef9f2c980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"LED_7k_epoch_5\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811f96be-e81e-436a-a11c-eefe7cdb9c84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-4ed7bfbb8faf>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4418645455988307\n",
      "rouge2:  0.16757827735014008\n",
      "rougeL:  0.24790280154672484\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=1)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8884fd-3bdf-4590-a9b4-a9b898ff6374",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_5.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fc237ea-8cc1-4bde-a5e6-4516da9e88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"LED_7k_epoch_5.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83404ea3-8706-42df-a387-539c9694c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"LED_7k_epoch_5.25\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44f05dd-8afe-42e0-bb13-91a87170c4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.43585895521476786\n",
      "rouge2:  0.1605719477937792\n",
      "rougeL:  0.2447807082040388\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e708d36-ee1e-44d2-9b8c-9c689e7986c7",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1108c8d9-03d6-4f58-b84c-4b005f63e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"LED_7k_epoch_5.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eaaff0f-c5d3-4f0a-b5ee-a93c323dfcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"LED_7k_epoch_5.5\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c9fd0b2-f557-4d50-803f-24f79fdcecb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4394120014536335\n",
      "rouge2:  0.16441785815924614\n",
      "rougeL:  0.24521893008188836\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06fcab9-7f08-4c1d-bf19-2ee918147076",
   "metadata": {},
   "source": [
    "# LED_7k_epoch_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "513405fa-7975-4fcc-ae17-cf5b6cc85d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LEDTokenizer.from_pretrained(\"LED_7k_epoch_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7cb5d91-d08a-4f06-a289-6d794bb59273",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\"LED_7k_epoch_6\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36f0ff65-fab2-4099-8c70-4062e33eb76c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.4392286662569279\n",
      "rouge2:  0.16368503033729087\n",
      "rougeL:  0.24642040678686372\n"
     ]
    }
   ],
   "source": [
    "result = data_test.map(generate_answer, batched=True, batch_size=4)\n",
    "rouge = load_metric(\"rouge\")\n",
    "score =  rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "print('rouge1: ', score['rouge1'].mid.fmeasure)\n",
    "print('rouge2: ', score['rouge2'].mid.fmeasure)\n",
    "print('rougeL: ', score['rougeL'].mid.fmeasure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
